{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3acfcc6",
   "metadata": {},
   "source": [
    "# 1. Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b716343",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030461ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import mediapipe as mp\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17ae81",
   "metadata": {},
   "source": [
    "# 2. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589e152",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/20bn-jester/Train.csv\")\n",
    "df.drop(labels = \"format\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c3a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf23b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0134bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95b3c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = [\"Doing other things\", \"No gesture\", 'Rolling Hand Backward', 'Rolling Hand Forward', 'Shaking Hand', \n",
    "           'Sliding Two Fingers Down', 'Sliding Two Fingers Left', 'Sliding Two Fingers Right', 'Sliding Two Fingers Up',\n",
    "            'Stop Sign', 'Swiping Down','Swiping Left', 'Swiping Right', 'Swiping Up',\n",
    "            'Thumb Down', 'Thumb Up',\n",
    "            'Turning Hand Clockwise', 'Turning Hand Counterclockwise'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f4c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df[df['label'].isin(actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1480d50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb5b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_stats = df_filtered.groupby(['label', 'label_id']).size().reset_index(name='count')\n",
    "print(label_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288da319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "train_dir = Path('/kaggle/input/20bn-jester/Train')\n",
    "\n",
    "# Lấy danh sách tất cả folder con\n",
    "all_folders = [f for f in train_dir.iterdir() if f.is_dir()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6951c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video = len(all_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1ad14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f918f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into action-specific subsets for parallel joint landmark extraction.\n",
    "\n",
    "def make_folder(action):\n",
    "    df_filtered = df[df['label'] == action]\n",
    "    video_ids = set(df_filtered['video_id'].astype(str))\n",
    "\n",
    "    #List of folder with the label == action    \n",
    "    filtered_folder = [f for f in all_folders if f.name in video_ids]\n",
    "    #Output is the POSIX PATH of the folder respective to action\n",
    "    return filtered_folder "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1f8353",
   "metadata": {},
   "source": [
    "# 3. Keypoint using mediapipe holistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic  #Download model -> make detections\n",
    "mp_drawing = mp.solutions.drawing_utils #Drawing utility -> draw keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PoseLandmark we use in this project\n",
    "from mediapipe.python.solutions.holistic import PoseLandmark\n",
    "included_landmarks = [\n",
    "    # right hand set\n",
    "    PoseLandmark.RIGHT_SHOULDER, \n",
    "    PoseLandmark.RIGHT_ELBOW, \n",
    "    PoseLandmark.RIGHT_WRIST, \n",
    "\n",
    "    # left hand set\n",
    "    PoseLandmark.LEFT_SHOULDER, \n",
    "    PoseLandmark.LEFT_ELBOW, \n",
    "    PoseLandmark.LEFT_WRIST, \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Processes an RGB image and returns the pose landmarks, left and right hand landmarks, and face landmarks on the most prominent person detected.\n",
    "    Returns:\n",
    "      A NamedTuple with fields describing the landmarks on the most prominate\n",
    "      person detected:\n",
    "        1) \"pose_landmarks\" field that contains the pose landmarks.\n",
    "        2) \"pose_world_landmarks\" field that contains the pose landmarks in\n",
    "        real-world 3D coordinates that are in meters with the origin at the\n",
    "        center between hips.\n",
    "        3) \"left_hand_landmarks\" field that contains the left-hand landmarks.\n",
    "        4) \"right_hand_landmarks\" field that contains the right-hand landmarks.\n",
    "        5) \"face_landmarks\" field that contains the face landmarks.\n",
    "        6) \"segmentation_mask\" field that contains the segmentation mask if\n",
    "           \"enable_segmentation\" is set to true.\n",
    "    \"\"\"\n",
    "def mediapipe_detection(image, model):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #Convert to RGB Color_space\n",
    "    image.flags.writeable = False #cvt numpy to read-only\n",
    "    res = model.process(image) #make prediction\n",
    "    image.flags.writeable = True\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "    return image, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921722fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, res):\n",
    "    #draw hand connections\n",
    "    mp_drawing.draw_landmarks(image, res.pose_landmarks,mp_holistic.POSE_CONNECTIONS) #draw pose connection \n",
    "    mp_drawing.draw_landmarks(image, res.left_hand_landmarks,mp_holistic.HAND_CONNECTIONS) #draw hand connection\n",
    "    mp_drawing.draw_landmarks(image, res.right_hand_landmarks,mp_holistic.HAND_CONNECTIONS) #draw hand connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9570891a",
   "metadata": {},
   "source": [
    "# 4. Extract keypoint features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24da77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoint(res):\n",
    "    arr = []\n",
    "\n",
    "    # Pose landmarks (6 points)\n",
    "    if res.pose_landmarks:\n",
    "        for landmark_id in included_landmarks:\n",
    "            point = res.pose_landmarks.landmark[landmark_id]\n",
    "            arr.extend([point.x, point.y, point.z])\n",
    "    else:\n",
    "        arr.extend([0, 0, 0] * len(included_landmarks))  # 6*3 = 18\n",
    "\n",
    "    # Left hand landmarks (21 points)\n",
    "    if res.left_hand_landmarks:\n",
    "        for point in res.left_hand_landmarks.landmark:\n",
    "            arr.extend([point.x, point.y, point.z])\n",
    "    else:\n",
    "        arr.extend([0, 0, 0] * 21)\n",
    "\n",
    "    # Right hand landmarks (21 points)\n",
    "    if res.right_hand_landmarks:\n",
    "        for point in res.right_hand_landmarks.landmark:\n",
    "            arr.extend([point.x, point.y, point.z])\n",
    "    else:\n",
    "        arr.extend([0, 0, 0] * 21)\n",
    "\n",
    "    return np.array(arr).reshape(48, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af73c92",
   "metadata": {},
   "source": [
    "## 4.1. Set Folder for Collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27483347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following section, we will process the training dataset; similar steps will be applied to the validation and test datasets.\n",
    "\n",
    "data_path = os.path.join(\"/kaggle/working/data/joint_stream/train\")\n",
    "os.makedirs(data_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a415f55",
   "metadata": {},
   "source": [
    "# 5. Collect keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8946b436",
   "metadata": {},
   "source": [
    "## 5.1. Joint stream dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5635ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of frames each video\n",
    "sequence_length = 37"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6f2b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_video(action, video_index, video_folder, data_path, sequence_length=37):\n",
    "    \"\"\"Process a video: read each frame, extract keypoints, and save them into one .npy file\"\"\"\n",
    "\n",
    "    sequence = []  # List to store keypoints for 37 frames from one video\n",
    "\n",
    "    # Initialize Mediapipe Holistic model to detect pose and hand landmarks\n",
    "    with mp.solutions.holistic.Holistic(\n",
    "        min_detection_confidence=0.5,\n",
    "        min_tracking_confidence=0.5,\n",
    "        model_complexity=1\n",
    "    ) as holistic:\n",
    "\n",
    "        frame_processed = 0  # Counter for successfully processed frames\n",
    "        for frame_num in range(1, sequence_length + 1):\n",
    "            frame_path = video_folder / f\"{frame_num:05d}.jpg\"  # Build path to the frame image\n",
    "\n",
    "            if not frame_path.exists():\n",
    "                continue  # Skip if the frame does not exist\n",
    "\n",
    "            frame = cv2.imread(str(frame_path))  # Read the frame\n",
    "\n",
    "            if frame is None:\n",
    "                continue  # Skip if the frame cannot be read\n",
    "\n",
    "            # Detect landmarks using Mediapipe\n",
    "            image, res = mediapipe_detection(frame, holistic)\n",
    "\n",
    "            # Extract keypoints from detection results\n",
    "            keypoints = extract_keypoint(res)\n",
    "            sequence.append(keypoints)\n",
    "            frame_processed += 1\n",
    "\n",
    "            # Release memory\n",
    "            del image, res\n",
    "\n",
    "        # Save extracted keypoints to a .npy file\n",
    "        npy_path = os.path.join(data_path, f\"{video_index}.npy\")\n",
    "        np.save(npy_path, np.array(sequence))\n",
    "\n",
    "    return action, video_index, frame_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358071d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Output structure:\n",
    "\n",
    "joint_stream/\n",
    "    ├── train/\n",
    "    │     ├── videoid1.npy\n",
    "    │     ├── videoid2.npy\n",
    "    │     └── ...\n",
    "    └── val/\n",
    "          ├── videoid1.npy\n",
    "          ├── videoid2.npy\n",
    "          └── ...\n",
    "\n",
    "Each .npy file has shape (T, V, C) = (37, 48, 3), where:\n",
    "    - T = 37 frames (temporal dimension)\n",
    "    - V = 48 joints (including 6 body pose landmarks and 21 landmarks for each left and right hand)\n",
    "    - C = 3 channels (x, y, z coordinates)\n",
    "\"\"\"\n",
    "\n",
    "num_workers = max(1, multiprocessing.cpu_count() - 1)\n",
    "action_videos = {}  \n",
    "total_videos = 0\n",
    "for action in actions: \n",
    "    filtered_folder = make_folder(action)\n",
    "    action_videos[action] = filtered_folder\n",
    "    total_videos += len(filtered_folder)\n",
    "    \n",
    "# Multi-thread Processing\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    futures = []\n",
    "\n",
    "    for action in actions:\n",
    "        for video_folder in action_videos[action]:\n",
    "            video_id = int(video_folder.name)\n",
    "            futures.append(\n",
    "                executor.submit(\n",
    "                    process_video, \n",
    "                    action,\n",
    "                    video_id, \n",
    "                    video_folder, \n",
    "                    data_path, \n",
    "                    sequence_length\n",
    "                )\n",
    "            )\n",
    "        \n",
    "    with tqdm(total=total_videos, desc=\"Processing all videos\") as pbar:\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                action, video_id, frames = future.result()\n",
    "                pbar.update(1)\n",
    "                pbar.set_description(f\"Processed: {action} - video {video_id} ({frames}/{sequence_length} frames)\")\n",
    "                \n",
    "                # Periodic memory cleanup\n",
    "                if pbar.n % 10 == 0:\n",
    "                    gc.collect()\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing video: {e}\")\n",
    "            # Periodic memory cleanup\n",
    "            if pbar.n % 10 == 0:\n",
    "                gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2f96b6",
   "metadata": {},
   "source": [
    "## 5.2. Joint motion stream, Bone stream, Bone motion stream Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac795df",
   "metadata": {},
   "outputs": [],
   "source": [
    "POSE_CONNECTIONS = frozenset([(11, 12), (11, 13), (13, 15), (12, 14), (14, 16)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from pose landmark IDs to file landmark IDs\n",
    "poseid2fileid = {12: 0, 14: 1, 16: 2, 11: 3, 13: 4, 15: 5}\n",
    "\n",
    "# Create file connections based on mapped pose IDs\n",
    "POSE_CONNECTIONS_FILE_INDEX = [\n",
    "    (poseid2fileid[a], poseid2fileid[b])\n",
    "    for (a, b) in POSE_CONNECTIONS\n",
    "]\n",
    "\n",
    "#[(3, 0), (3, 4), (4, 5), (0, 1), (1, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b453d44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The 21 hand landmarks:\n",
    "  WRIST = 0\n",
    "  THUMB_CMC = 1\n",
    "  THUMB_MCP = 2\n",
    "  THUMB_IP = 3\n",
    "  THUMB_TIP = 4\n",
    "  INDEX_FINGER_MCP = 5\n",
    "  INDEX_FINGER_PIP = 6\n",
    "  INDEX_FINGER_DIP = 7\n",
    "  INDEX_FINGER_TIP = 8\n",
    "  MIDDLE_FINGER_MCP = 9\n",
    "  MIDDLE_FINGER_PIP = 10\n",
    "  MIDDLE_FINGER_DIP = 11\n",
    "  MIDDLE_FINGER_TIP = 12\n",
    "  RING_FINGER_MCP = 13\n",
    "  RING_FINGER_PIP = 14\n",
    "  RING_FINGER_DIP = 15\n",
    "  RING_FINGER_TIP = 16\n",
    "  PINKY_MCP = 17\n",
    "  PINKY_PIP = 18\n",
    "  PINKY_DIP = 19\n",
    "  PINKY_TIP = 20\n",
    "\"\"\"\n",
    "\n",
    "\"Connection\" \n",
    "HAND_PALM_CONNECTIONS = ((0, 1), (0, 5), (9, 13), (13, 17), (5, 9), (0, 17))\n",
    "\n",
    "HAND_THUMB_CONNECTIONS = ((1, 2), (2, 3), (3, 4))\n",
    "\n",
    "HAND_INDEX_FINGER_CONNECTIONS = ((5, 6), (6, 7), (7, 8))\n",
    "\n",
    "HAND_MIDDLE_FINGER_CONNECTIONS = ((9, 10), (10, 11), (11, 12))\n",
    "\n",
    "HAND_RING_FINGER_CONNECTIONS = ((13, 14), (14, 15), (15, 16))\n",
    "\n",
    "HAND_PINKY_FINGER_CONNECTIONS = ((17, 18), (18, 19), (19, 20))\n",
    "\n",
    "HAND_CONNECTIONS = frozenset().union(*[\n",
    "    HAND_PALM_CONNECTIONS, HAND_THUMB_CONNECTIONS,\n",
    "    HAND_INDEX_FINGER_CONNECTIONS, HAND_MIDDLE_FINGER_CONNECTIONS,\n",
    "    HAND_RING_FINGER_CONNECTIONS, HAND_PINKY_FINGER_CONNECTIONS\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13924872",
   "metadata": {},
   "outputs": [],
   "source": [
    "handid2fileid = {a: a + 6 for a in range(21)}\n",
    "print(handid2fileid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001084ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping from joint_id to index_id of array\n",
    "\n",
    "LEFT_HAND_CONNECTIONS_FILE_INDEX = [\n",
    "    (handid2fileid[a], handid2fileid[b])\n",
    "    for (a, b) in HAND_CONNECTIONS\n",
    "]\n",
    "\n",
    "RIGHT_HAND_CONNECTIONS_FILE_INDEX = [\n",
    "    (handid2fileid[a] + 21, handid2fileid[b] + 21)\n",
    "    for (a, b) in HAND_CONNECTIONS\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07340dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source paths from joint stream dataset\n",
    "train_src = \"/kaggle/input/msstnetdataset/data/joint_stream/train\"\n",
    "val_src = \"/kaggle/input/msstnetdataset/data/joint_stream/val\"\n",
    "\n",
    "# Destination paths for processed datasets:\n",
    "# - joint_motion_stream: differences between consecutive frames of joint landmarks: (T-1, V, C)\n",
    "# - bone_stream: vectors between connected joints (pose and hands): (T, V-1, C)\n",
    "# - bone_motion_stream: differences between consecutive frames of bone vectors (T-1, V-1, C)\n",
    "\n",
    "train_dst1 = \"/kaggle/working/data/joint_motion_stream/train\"\n",
    "val_dst1 = \"/kaggle/working/data/joint_motion_stream/val\"\n",
    "train_dst2 = \"/kaggle/working/data/bone_stream/train\"\n",
    "val_dst2 = \"/kaggle/working/data/bone_stream/val\"\n",
    "train_dst3 = \"/kaggle/working/data/bone_motion_stream/train\"\n",
    "val_dst3 = \"/kaggle/working/data/bone_motion_stream/val\"\n",
    "\n",
    "os.makedirs(train_dst1, exist_ok=True)\n",
    "os.makedirs(val_dst1, exist_ok=True)\n",
    "os.makedirs(train_dst2, exist_ok=True)\n",
    "os.makedirs(val_dst2, exist_ok=True)\n",
    "os.makedirs(train_dst3, exist_ok=True)\n",
    "os.makedirs(val_dst3, exist_ok=True)\n",
    "\n",
    "def process_in_batches(src_folder, dst_folder1, dst_folder2, dst_folder3, batch_size=100):\n",
    "    all_files = [f for f in os.listdir(src_folder) if f.endswith(\".npy\")]\n",
    "    total_files = len(all_files)\n",
    "    \n",
    "    for i in range(0, total_files, batch_size):\n",
    "        batch_files = all_files[i:min(i+batch_size, total_files)]\n",
    "        print(f\"Processing batch {i//batch_size + 1}/{(total_files + batch_size - 1)//batch_size} ({len(batch_files)} files)\")\n",
    "        \n",
    "        for filename in tqdm(batch_files):\n",
    "            try:\n",
    "                src_file = os.path.join(src_folder, filename)\n",
    "                file = np.load(src_file)  # Load the original (T, V, C) array\n",
    "                \n",
    "                # 1. Joint motion stream\n",
    "                dst_file1 = os.path.join(dst_folder1, filename)\n",
    "                file1 = file[1:] - file[:-1]\n",
    "                np.save(dst_file1, file1)\n",
    "                \n",
    "                # 2. Bone stream\n",
    "                bone_vector = []\n",
    "                def retrieve_bone_vector(x):\n",
    "                    return np.stack([file[:, b, :] - file[:, a, :] for (a, b) in x], axis=1)\n",
    "                \n",
    "                bone_pose = retrieve_bone_vector(POSE_CONNECTIONS_FILE_INDEX)\n",
    "                bone_left = retrieve_bone_vector(LEFT_HAND_CONNECTIONS_FILE_INDEX)\n",
    "                bone_right = retrieve_bone_vector(RIGHT_HAND_CONNECTIONS_FILE_INDEX)\n",
    "                \n",
    "                if bone_pose is None or bone_left is None or bone_right is None:\n",
    "                    continue\n",
    "                    \n",
    "                bone_vector = np.concatenate([bone_pose, bone_left, bone_right], axis=1)\n",
    "                \n",
    "                dst_file2 = os.path.join(dst_folder2, filename)\n",
    "                np.save(dst_file2, bone_vector)\n",
    "                \n",
    "                # 3. Bone motion stream\n",
    "                file3 = bone_vector[1:] - bone_vector[:-1]\n",
    "                dst_file3 = os.path.join(dst_folder3, filename)\n",
    "                np.save(dst_file3, file3)\n",
    "                \n",
    "                # Release memory after each file\n",
    "                del file, file1, bone_vector, file3, bone_pose, bone_left, bone_right\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "print(\"Processing training data...\")\n",
    "process_in_batches(train_src, train_dst1, train_dst2, train_dst3)\n",
    "\n",
    "print(\"Processing validation data...\")\n",
    "process_in_batches(val_src, val_dst1, val_dst2, val_dst3)\n",
    "\n",
    "print(\"Data processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8693db",
   "metadata": {},
   "source": [
    "## 5.3. Create x, y for Training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea914b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {action: idx for idx, action in enumerate(actions)}  # actions = list of 18 action name\n",
    "\n",
    "def data_train(folder, path_csv):\n",
    "    x = []\n",
    "    y = []\n",
    "    \n",
    "    df = pd.read_csv(path_csv)\n",
    "    df.drop(columns = ['format', 'shape'], inplace = True)\n",
    "    df_filtered = df[df['label'].isin(actions)]\n",
    "    \n",
    "    for f in folder.iterdir():\n",
    "        if f.is_file() and f.suffix == \".npy\":\n",
    "            # x\n",
    "            x.append(np.load(f.as_posix()))\n",
    "\n",
    "            # y\n",
    "            video_id = int(f.stem)\n",
    "            label = df_filtered.loc[df_filtered['video_id'] == video_id, 'label'].values\n",
    "            y.append(label2id[label[0]])\n",
    "            \n",
    "    y = to_categorical(y, num_classes = len(actions)).astype(int)\n",
    "    return np.array(x, dtype = np.float32), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dafa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the same steps to train and val sets of joint motion, bone, and bone motion streams.\n",
    "\n",
    "x_train, y_train = data_train(Path(\"/kaggle/input/msstnetdataset/data/joint_stream/train\"),\"/kaggle/input/20bn-jester/Train.csv\")\n",
    "x_val, y_val = data_train(Path(\"/kaggle/input/msstnetdataset/data/joint_stream/val\"),\"/kaggle/input/20bn-jester/Validation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43ad5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431fe066",
   "metadata": {},
   "source": [
    "# 6. Build Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0adda07",
   "metadata": {},
   "source": [
    "## 6.1. Setting Up Checkpoints and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df740bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Concatenate, Conv2D, Dense, InputLayer, Activation, GlobalAveragePooling2D, BatchNormalization, ReLU, AveragePooling2D, Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a871c485",
   "metadata": {},
   "outputs": [],
   "source": [
    "KAGGLE_WORKING = '/kaggle/working'  # Path for output files\n",
    "\n",
    "checkpoint_dir = os.path.join(KAGGLE_WORKING, 'checkpoints')\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "checkpoint_path = os.path.join(checkpoint_dir, 'model_epoch_{epoch:03d}_acc_{val_categorical_acc: .4f}.keras')\n",
    "best_model_path = os.path.join(checkpoint_dir, 'best_model.keras')\n",
    "log_dir = os.path.join(KAGGLE_WORKING, 'logs', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa46ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_latest_checkpoint():\n",
    "    checkpoint_files = glob.glob(os.path.join(checkpoint_dir, 'model_epoch_*.keras'))\n",
    "    if not checkpoint_files:\n",
    "        return None\n",
    "    \n",
    "    # Sort by epoch number\n",
    "    checkpoint_files.sort(key=lambda x: int(x.split('_epoch_')[1].split('_')[0]))\n",
    "    return checkpoint_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31b05f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for existing checkpoint to resume training\n",
    "latest_checkpoint = find_latest_checkpoint()\n",
    "initial_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d2035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if latest_checkpoint:\n",
    "    print(f\"Found checkpoint: {latest_checkpoint}\")\n",
    "    try:\n",
    "        # Extract epoch number from checkpoint filename\n",
    "        epoch_str = os.path.basename(latest_checkpoint).split('_epoch_')[1].split('_')[0]\n",
    "        initial_epoch = int(epoch_str)\n",
    "        print(f\"Resuming from epoch {initial_epoch}\")\n",
    "        \n",
    "        # Load the model\n",
    "        model = load_model(latest_checkpoint)\n",
    "        print(\"Model loaded successfully from checkpoint\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading checkpoint: {e}\")\n",
    "        print(\"Creating new model instead\")\n",
    "        latest_checkpoint = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f48366",
   "metadata": {},
   "source": [
    "## 6.2. Define MSST (Multi-Scale Spatio-Temporal) Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f22b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MSST (Multi-Scale Spatio-Temporal) Module\n",
    "\n",
    "Architecture:\n",
    "- Input is fed into 5 parallel branches\n",
    "- Each branch begins with a 1×1 convolution to reduce dimensions\n",
    "- Branches have different convolutional patterns:\n",
    "  * Branch 1: Simple 1×1 convolution (preserves spatial information)\n",
    "  * Branch 2: 1×1 followed by separate 3×1 and 1×3 convolutions (small receptive field)\n",
    "  * Branch 3: 1×1 followed by separate 5×1 and 1×5 convolutions (medium receptive field)\n",
    "  * Branch 4: 1×1 followed by separate 7×1 and 1×7 convolutions (larger receptive field)\n",
    "  * Branch 5: 1×1 followed by separate 11×1 and 1×11 convolutions (largest receptive field)\n",
    "- All branches are concatenated at the end to produce output feature maps\n",
    "\n",
    "Each convolution is typically followed by batch normalization and ReLU activation. This design efficiently captures multi-scale spatial information\n",
    "\"\"\"\n",
    "\n",
    "class MSST_Layer(Layer):\n",
    "    def __init__(self, stride, filter1, filter2, filter3, filter4, filter5, **kwargs):\n",
    "        super(MSST_Layer, self).__init__(**kwargs)\n",
    "        self.stride = stride\n",
    "        self.filters = [filter1, filter2, filter3, filter4, filter5]\n",
    "        self.concat = Concatenate()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.branch1 = self._make_branch([(1, 1)], self.filters[0], self.stride)\n",
    "        self.branch2 = self._make_branch([(1, 1), (3, 3)], [self.filters[0], self.filters[1]], self.stride)\n",
    "        self.branch3 = self._make_branch([(1, 1), (5, 1), (1, 5)], [self.filters[2]]*3, self.stride)\n",
    "        self.branch4 = self._make_branch([(1, 1), (7, 1), (1, 7)], [self.filters[3]]*3, self.stride)\n",
    "        self.branch5 = self._make_branch([(1, 1), (11, 1), (1, 11)], [self.filters[4]]*3, self.stride)\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def _make_branch(self, kernel_sizes, filters, first_stride):\n",
    "        layers = []\n",
    "        if isinstance(filters, int):\n",
    "            filters = [filters] * len(kernel_sizes)\n",
    "        for i, (kernel, f) in enumerate(zip(kernel_sizes, filters)):\n",
    "            stride = first_stride if i == 0 else (1, 1)\n",
    "            layers.append(Conv2D(f, kernel_size=kernel, strides=stride, padding='same'))\n",
    "            layers.append(BatchNormalization())\n",
    "            layers.append(ReLU())\n",
    "        return layers\n",
    "\n",
    "    def _apply_branch(self, inputs, branch_layers, training):\n",
    "        x = inputs\n",
    "        for layer in branch_layers:\n",
    "            if isinstance(layer, BatchNormalization):\n",
    "                x = layer(x, training=training)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        b1 = self._apply_branch(inputs, self.branch1, training)\n",
    "        b2 = self._apply_branch(inputs, self.branch2, training)\n",
    "        b3 = self._apply_branch(inputs, self.branch3, training)\n",
    "        b4 = self._apply_branch(inputs, self.branch4, training)\n",
    "        b5 = self._apply_branch(inputs, self.branch5, training)\n",
    "        return self.concat([b1, b2, b3, b4, b5])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MSST_Layer, self).get_config()\n",
    "        keys = ['filter1', 'filter2', 'filter3', 'filter4', 'filter5']\n",
    "        config.update({'stride': self.stride, **{k: v for k, v in zip(keys, self.filters)}})\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107760d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Architecture of the MSSTNet-based Action Recognition: \n",
    "# Train 4 models using 4 datasets: joint stream, joint motion stream, bone stream, and bone motion stream. \n",
    "# Then, use ensemble learning to combine the results of each model by averaging their weights.\n",
    "# In the following section, we will process the joint stream dataset. Similar steps will be applied to the other datasets.\n",
    "\n",
    "input_shape = (37, 48, 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748eb9a5",
   "metadata": {},
   "source": [
    "## 6.3. Define MSSTNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4998a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not latest_checkpoint:\n",
    "    print(\"Creating new model\")\n",
    "    model = Sequential()\n",
    "    model.add(InputLayer(input_shape = input_shape))\n",
    "    \n",
    "    model.add(MSST_Layer(stride=(1,1), filter1=44, filter2=60, filter3=60, filter4=60,filter5=60))\n",
    "    model.add(MSST_Layer(stride=(1,1), filter1=48, filter2=80, filter3=80, filter4=80,filter5=80))\n",
    "    model.add(MSST_Layer(stride=(1,1), filter1=56, filter2=120, filter3=120, filter4=120,filter5=120))\n",
    "    \n",
    "    model.add(AveragePooling2D(pool_size=(3,3), strides=(1,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(MSST_Layer(stride=(1,1), filter1=160, filter2=160, filter3=160, filter4=160,filter5=160))\n",
    "    model.add(MSST_Layer(stride=(2,1), filter1=72, filter2=200, filter3=200, filter4=200,filter5=200))\n",
    "    \n",
    "    model.add(AveragePooling2D(pool_size=(3,3), strides=(1,2)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(MSST_Layer(stride=(1,1), filter1=240, filter2=240, filter3=240, filter4=240,filter5=240))\n",
    "    model.add(MSST_Layer(stride=(1,1), filter1=320, filter2=320, filter3=320, filter4=320,filter5=320))\n",
    "    \n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    model.add(Dense(18, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc4b2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "\n",
    "optimizer = Adam(learning_rate = 0.01)\n",
    "model.compile(optimizer = optimizer,\n",
    "             loss = 'categorical_crossentropy',\n",
    "             metrics = ['categorical_accuracy']\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa00a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e66067c",
   "metadata": {},
   "source": [
    "# 7. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b4e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom callback to save model every 5 epochs\n",
    "class PeriodicSaver(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, save_path, every=5):\n",
    "        super().__init__()\n",
    "        self.save_path = save_path\n",
    "        self.every = every\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if (epoch + 1) % self.every == 0:\n",
    "            val_acc = logs.get('val_categorical_accuracy', 0)\n",
    "            filename = self.save_path.format(epoch=epoch + 1, val_categorical_acc=val_acc)\n",
    "            self.model.save(filename)\n",
    "            print(f\"Saved periodic model at epoch {epoch + 1} to {filename}\")\n",
    "\n",
    "# Callbacks for optimized training\n",
    "callbacks = [\n",
    "    # Custom checkpoint every 5 epochs\n",
    "    PeriodicSaver(\n",
    "        save_path=checkpoint_path,\n",
    "        every=5\n",
    "    ),\n",
    "    \n",
    "    # Best model checkpoint\n",
    "    ModelCheckpoint(\n",
    "        filepath=best_model_path,\n",
    "        monitor='val_categorical_accuracy',\n",
    "        verbose=1,\n",
    "        save_best_only=True,\n",
    "        mode='max'\n",
    "    ),\n",
    "    \n",
    "    # Stop training when validation accuracy doesn't improve\n",
    "    EarlyStopping(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        patience=7,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # Reduce learning rate when plateau is reached\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_categorical_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=4,\n",
    "        min_lr=1e-5,\n",
    "        verbose=1\n",
    "    ),\n",
    "    \n",
    "    # TensorBoard visualization\n",
    "    TensorBoard(log_dir=log_dir)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30       \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c42868c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_val, y_val),\n",
    "    epochs=epochs,\n",
    "    initial_epoch=initial_epoch,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1241c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['categorical_accuracy'], label='Train')\n",
    "plt.plot(history.history['val_categorical_accuracy'], label='Validation')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(KAGGLE_WORKING, 'training_history.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142a0c68",
   "metadata": {},
   "source": [
    "# 8. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects={'MSST_Layer': MSST_Layer}\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_path1 = \"path/to/best_model_for_joint_stream.keras\"\n",
    "model_path2 = \"path/to/best_model_for_joint_motion_stream.keras\"\n",
    "model_path3 = \"path/to/best_model_for_bone_stream.keras\"\n",
    "model_path4 = \"path/to/best_model_for_bone_motion_stream.keras\"\n",
    "\n",
    "# Load models\n",
    "model1 = load_model(model_path1)\n",
    "model2 = load_model(model_path2)\n",
    "model3 = load_model(model_path3)\n",
    "model4 = load_model(model_path4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703b3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need output shape: (1, 37, 48, 3)\n",
    "def output_process(frames, model):\n",
    "    sequence = []\n",
    "    \n",
    "    # Process each frame to extract keypoints\n",
    "    for frame_num in range(0, sequence_length):\n",
    "        frame = frames[frame_num]\n",
    "        if frame is None:\n",
    "            print(\"IO Error\")\n",
    "            continue\n",
    "        \n",
    "        image, res = mediapipe_detection(frame, model)\n",
    "        keypoints = extract_keypoint(res) \n",
    "        sequence.append(keypoints)  # Shape: (sequence_length, 48, 3)\n",
    "    \n",
    "    # 1. Joint stream\n",
    "    sequence = np.array(sequence)\n",
    "    sequence1 = np.expand_dims(sequence, axis = 0) # Shape: (1, sequence_length, 48, 3)\n",
    "\n",
    "    # 2. Join motion stream\n",
    "    sequence2 = sequence[1:] -sequence[:-1] # Shape: (sequence_length - 1, 48, 3)\n",
    "    \n",
    "    # 3. Bone stream\n",
    "    sequence3 = []\n",
    "\n",
    "    def retrieve_bone_vector(x):\n",
    "        return np.stack([sequence[:, b, :] - sequence[:, a, :] for (a, b) in x], axis=1)\n",
    "    \n",
    "    bone_pose = retrieve_bone_vector(POSE_CONNECTIONS_FILE_INDEX)\n",
    "    bone_left = retrieve_bone_vector(LEFT_HAND_CONNECTIONS_FILE_INDEX)\n",
    "    bone_right = retrieve_bone_vector(RIGHT_HAND_CONNECTIONS_FILE_INDEX)    \n",
    "    sequence3 = np.concatenate([bone_pose, bone_left, bone_right], axis=1) # Shape: (sequence_length, 47, 3)\n",
    "\n",
    "    # 4. Bone motion stream\n",
    "    sequence4 = sequence3[1:] - sequence3[:-1] # Shape: (sequence_length - 1, 47, 3)\n",
    "\n",
    "    return np.expand_dims(sequence, axis = 0), np.expand_dims(sequence2, axis = 0), np.expand_dims(sequence3, axis = 0), np.expand_dims(sequence4, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2cc240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# List to store captured frames\n",
    "frames = []\n",
    "\n",
    "# Flag to indicate whether we are currently recording frames\n",
    "is_recording = False\n",
    "\n",
    "print(\"Press 's' to start capturing, and 'q' to stop capturing.\")\n",
    "\n",
    "# Initialize Mediapipe Holistic model\n",
    "with mp.solutions.holistic.Holistic(\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    model_complexity=1\n",
    ") as holistic:\n",
    "    while True:\n",
    "        # Capture a frame from the webcam\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # If recording, save the current frame\n",
    "        if is_recording:\n",
    "            frames.append(frame)\n",
    "\n",
    "        # Show the current frame\n",
    "        cv2.imshow(\"Webcam\", frame)\n",
    "\n",
    "        # Wait for a key press\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Start recording when 's' is pressed\n",
    "        if key == ord('s'):\n",
    "            is_recording = True\n",
    "        # Stop recording and exit when 'q' is pressed\n",
    "        elif key == ord('q'):\n",
    "            if is_recording:\n",
    "                break\n",
    "\n",
    "    # Release the webcam and close all OpenCV windows\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Downsample frames to 37 frames if necessary\n",
    "    total_frames = len(frames)\n",
    "    step = max(1, total_frames // 37)\n",
    "    frames = [frames[i] for i in range(0, total_frames, step)][:37]\n",
    "\n",
    "    # If not enough frames, duplicate the last frame\n",
    "    while len(frames) < 37:\n",
    "        frames.append(frames[-1])\n",
    "\n",
    "    # x_test for each model\n",
    "    sequence1, sequence2, sequence3, sequence4 = output_process(frames, holistic)\n",
    "\n",
    "    def predict(model, sequence, name):\n",
    "        y = model.predict(sequence)\n",
    "        print(f\"{name} model: {actions[np.argmax(y)]} with predicted index: {np.argmax(y)}\")\n",
    "        print(\"***\")\n",
    "    \n",
    "    # Predict using each stream-specific model\n",
    "    predict(model1, sequence1, \"joint stream\")\n",
    "    predict(model2, sequence2, \"joint motion stream\")\n",
    "    predict(model3, sequence3, \"bone stream\")\n",
    "    predict(model4, sequence4, \"bone motion stream\")\n",
    "\n",
    "    # Ensemble prediction: average the outputs of all models\n",
    "    avg_percent = 1/4 * (\n",
    "        model1.predict(sequence1) + \n",
    "        model2.predict(sequence2) + \n",
    "        model3.predict(sequence3) + \n",
    "        model4.predict(sequence4)\n",
    "    )\n",
    "\n",
    "    print(f\"Assemble model: {actions[np.argmax(avg_percent)]} with predicted index: {np.argmax(avg_percent)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
