{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:46:01.195496Z",
     "iopub.status.busy": "2025-05-24T10:46:01.195235Z",
     "iopub.status.idle": "2025-05-24T10:46:08.416827Z",
     "shell.execute_reply": "2025-05-24T10:46:08.415850Z",
     "shell.execute_reply.started": "2025-05-24T10:46:01.195476Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:46:08.418888Z",
     "iopub.status.busy": "2025-05-24T10:46:08.418654Z",
     "iopub.status.idle": "2025-05-24T10:47:31.637108Z",
     "shell.execute_reply": "2025-05-24T10:47:31.636098Z",
     "shell.execute_reply.started": "2025-05-24T10:46:08.418865Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install ptflops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:47:31.638643Z",
     "iopub.status.busy": "2025-05-24T10:47:31.638429Z",
     "iopub.status.idle": "2025-05-24T10:47:38.040045Z",
     "shell.execute_reply": "2025-05-24T10:47:38.039317Z",
     "shell.execute_reply.started": "2025-05-24T10:47:31.638621Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class STMEM(nn.Module):\n",
    "    def __init__(self, num_segments, new_length, img_size=(224, 224)):\n",
    "        super(STMEM, self).__init__()\n",
    "        self.num_segments = num_segments\n",
    "        self.new_length = new_length\n",
    "        self.height, self.width = img_size\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.m1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=(self.new_length * 2 - 1) * 3,\n",
    "                out_channels=3,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.m2 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3,\n",
    "                out_channels=3,\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape: (B, S * L * 3, H, W)\n",
    "        B, SLC, H, W = x.size()\n",
    "        assert H == self.height and W == self.width, \\\n",
    "            f\"Expected input height={self.height}, width={self.width} but got {H}x{W}\"\n",
    "\n",
    "        # Reshape to: (B * S, L * 3, H, W)\n",
    "        x = x.view(B * self.num_segments, self.new_length * 3, self.height, self.width)\n",
    "\n",
    "        # Compute frame differences (temporal modeling)\n",
    "        frame_diff = x[:, 3:] - x[:, : (self.new_length - 1) * 3]\n",
    "\n",
    "        # Concatenate original input with motion information\n",
    "        x_with_diff = torch.cat((x, frame_diff), dim=1)\n",
    "        x_with_diff = self.m1(x_with_diff)\n",
    "\n",
    "        # Get max motion frame\n",
    "        frame_diff = frame_diff.view(B * self.num_segments, self.new_length - 1, 3, self.height, self.width)\n",
    "        frame_diff = frame_diff.max(dim=1)[0]\n",
    "\n",
    "        frame_diff = self.m2(frame_diff)\n",
    "        motion_mask = self.sigmoid(frame_diff)\n",
    "\n",
    "        # Apply motion mask\n",
    "        output = motion_mask * x_with_diff\n",
    "\n",
    "        return output  # shape: (B * S, 3, H, W)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    a = torch.rand([4,90,224,224])\n",
    "    model = STMEM(num_segments=5,new_length=6)\n",
    "    out = model(a)\n",
    "    print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:47:38.041951Z",
     "iopub.status.busy": "2025-05-24T10:47:38.041615Z",
     "iopub.status.idle": "2025-05-24T10:47:38.064061Z",
     "shell.execute_reply": "2025-05-24T10:47:38.063251Z",
     "shell.execute_reply.started": "2025-05-24T10:47:38.041930Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalShift(nn.Module):\n",
    "    def __init__(self, net, n_segment=3, n_div=8, inplace=False):\n",
    "        super(TemporalShift, self).__init__()\n",
    "        self.net = net\n",
    "        self.n_segment = n_segment\n",
    "        self.fold_div = n_div\n",
    "        self.inplace = inplace\n",
    "        if inplace:\n",
    "            print('=> Using in-place shift...')\n",
    "        print('=> Using fold div: {}'.format(self.fold_div))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.shift(x, self.n_segment, fold_div=self.fold_div, inplace=self.inplace)\n",
    "        return self.net(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def shift(x, n_segment, fold_div=3, inplace=False):\n",
    "        nt, c, h, w = x.size()\n",
    "        n_batch = nt // n_segment\n",
    "        x = x.view(n_batch, n_segment, c, h, w)\n",
    "\n",
    "        fold = c // fold_div\n",
    "        if inplace:\n",
    "            # Due to some out of order error when performing parallel computing. \n",
    "            # May need to write a CUDA kernel.\n",
    "            raise NotImplementedError  \n",
    "            # out = InplaceShift.apply(x, fold)\n",
    "        else:\n",
    "            out = torch.zeros_like(x)\n",
    "            out[:, :-1, :fold] = x[:, 1:, :fold]  # shift left\n",
    "            out[:, 1:, fold: 2 * fold] = x[:, :-1, fold: 2 * fold]  # shift right\n",
    "            out[:, :, 2 * fold:] = x[:, :, 2 * fold:]  # not shift\n",
    "\n",
    "        return out.view(nt, c, h, w)\n",
    "\n",
    "\n",
    "class InplaceShift(torch.autograd.Function):\n",
    "    # Special thanks to @raoyongming for the help to this function\n",
    "    @staticmethod\n",
    "    def forward(ctx, input, fold):\n",
    "        # not support higher order gradient\n",
    "        # input = input.detach_()\n",
    "        ctx.fold_ = fold\n",
    "        n, t, c, h, w = input.size()\n",
    "        buffer = input.data.new(n, t, fold, h, w).zero_()\n",
    "        buffer[:, :-1] = input.data[:, 1:, :fold]\n",
    "        input.data[:, :, :fold] = buffer\n",
    "        buffer.zero_()\n",
    "        buffer[:, 1:] = input.data[:, :-1, fold: 2 * fold]\n",
    "        input.data[:, :, fold: 2 * fold] = buffer\n",
    "        return input\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # grad_output = grad_output.detach_()\n",
    "        fold = ctx.fold_\n",
    "        n, t, c, h, w = grad_output.size()\n",
    "        buffer = grad_output.data.new(n, t, fold, h, w).zero_()\n",
    "        buffer[:, 1:] = grad_output.data[:, :-1, :fold]\n",
    "        grad_output.data[:, :, :fold] = buffer\n",
    "        buffer.zero_()\n",
    "        buffer[:, :-1] = grad_output.data[:, 1:, fold: 2 * fold]\n",
    "        grad_output.data[:, :, fold: 2 * fold] = buffer\n",
    "        return grad_output, None\n",
    "\n",
    "\n",
    "class TemporalPool(nn.Module):\n",
    "    def __init__(self, net, n_segment):\n",
    "        super(TemporalPool, self).__init__()\n",
    "        self.net = net\n",
    "        self.n_segment = n_segment\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.temporal_pool(x, n_segment=self.n_segment)\n",
    "        return self.net(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def temporal_pool(x, n_segment):\n",
    "        nt, c, h, w = x.size()\n",
    "        n_batch = nt // n_segment\n",
    "        x = x.view(n_batch, n_segment, c, h, w).transpose(1, 2)  # n, c, t, h, w\n",
    "        x = F.max_pool3d(x, kernel_size=(3, 1, 1), stride=(2, 1, 1), padding=(1, 0, 0))\n",
    "        x = x.transpose(1, 2).contiguous().view(nt // 2, c, h, w)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_temporal_shift(net, n_segment, n_div=8, place='blockres', temporal_pool=False):\n",
    "    if temporal_pool:\n",
    "        n_segment_list = [n_segment, n_segment // 2, n_segment // 2, n_segment // 2]\n",
    "    else:\n",
    "        n_segment_list = [n_segment] * 4\n",
    "    assert n_segment_list[-1] > 0\n",
    "    print('=> n_segment per stage: {}'.format(n_segment_list))\n",
    "\n",
    "    import torchvision\n",
    "    # if isinstance(net, resnet.ResNet):\n",
    "    if isinstance(net, torchvision.models.ResNet):\n",
    "        if place == 'block':\n",
    "            def make_block_temporal(stage, this_segment):\n",
    "                blocks = list(stage.children())\n",
    "                print('=> Processing stage with {} blocks'.format(len(blocks)))\n",
    "                for i, b in enumerate(blocks):\n",
    "                    blocks[i] = TemporalShift(b, n_segment=this_segment, n_div=n_div)\n",
    "                return nn.Sequential(*(blocks))\n",
    "\n",
    "            net.layer1 = make_block_temporal(net.layer1, n_segment_list[0])\n",
    "            net.layer2 = make_block_temporal(net.layer2, n_segment_list[1])\n",
    "            net.layer3 = make_block_temporal(net.layer3, n_segment_list[2])\n",
    "            net.layer4 = make_block_temporal(net.layer4, n_segment_list[3])\n",
    "\n",
    "        elif 'blockres' in place:\n",
    "            n_round = 1\n",
    "            if len(list(net.layer3.children())) >= 23:\n",
    "                n_round = 2\n",
    "                print('=> Using n_round {} to insert temporal shift'.format(n_round))\n",
    "\n",
    "            def make_block_temporal(stage, this_segment):\n",
    "                blocks = list(stage.children())\n",
    "                print('=> Processing stage with {} blocks residual'.format(len(blocks)))\n",
    "                for i, b in enumerate(blocks):\n",
    "                    if i % n_round == 0:\n",
    "                        blocks[i].conv1 = TemporalShift(b.conv1, n_segment=this_segment, n_div=n_div)\n",
    "                return nn.Sequential(*blocks)\n",
    "\n",
    "            net.layer1 = make_block_temporal(net.layer1, n_segment_list[0])\n",
    "            net.layer2 = make_block_temporal(net.layer2, n_segment_list[1])\n",
    "            net.layer3 = make_block_temporal(net.layer3, n_segment_list[2])\n",
    "            net.layer4 = make_block_temporal(net.layer4, n_segment_list[3])\n",
    "    else:\n",
    "        raise NotImplementedError(place)\n",
    "\n",
    "\n",
    "def make_temporal_pool(net, n_segment):\n",
    "    import torchvision\n",
    "    if isinstance(net, torchvision.models.ResNet):\n",
    "        print('=> Injecting nonlocal pooling')\n",
    "        net.layer2 = TemporalPool(net.layer2, n_segment)\n",
    "    else:\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:47:38.065220Z",
     "iopub.status.busy": "2025-05-24T10:47:38.064970Z",
     "iopub.status.idle": "2025-05-24T10:47:41.195371Z",
     "shell.execute_reply": "2025-05-24T10:47:41.194653Z",
     "shell.execute_reply.started": "2025-05-24T10:47:38.065195Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Non-local block using embedded gaussian\n",
    "# Code from\n",
    "# https://github.com/AlexHex7/Non-local_pytorch/blob/master/Non-Local_pytorch_0.3.1/lib/non_local_embedded_gaussian.py\n",
    "class _NonLocalBlockND(nn.Module):\n",
    "    def __init__(self, in_channels, inter_channels=None, dimension=3, sub_sample=True, bn_layer=True):\n",
    "        super(_NonLocalBlockND, self).__init__()\n",
    "\n",
    "        assert dimension in [1, 2, 3]\n",
    "\n",
    "        self.dimension = dimension\n",
    "        self.sub_sample = sub_sample\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.inter_channels = inter_channels\n",
    "\n",
    "        if self.inter_channels is None:\n",
    "            self.inter_channels = in_channels // 2\n",
    "            if self.inter_channels == 0:\n",
    "                self.inter_channels = 1\n",
    "\n",
    "        if dimension == 3:\n",
    "            conv_nd = nn.Conv3d\n",
    "            max_pool_layer = nn.MaxPool3d(kernel_size=(1, 2, 2))\n",
    "            bn = nn.BatchNorm3d\n",
    "        elif dimension == 2:\n",
    "            conv_nd = nn.Conv2d\n",
    "            max_pool_layer = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "            bn = nn.BatchNorm2d\n",
    "        else:\n",
    "            conv_nd = nn.Conv1d\n",
    "            max_pool_layer = nn.MaxPool1d(kernel_size=(2))\n",
    "            bn = nn.BatchNorm1d\n",
    "\n",
    "        self.g = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                         kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if bn_layer:\n",
    "            self.W = nn.Sequential(\n",
    "                conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                        kernel_size=1, stride=1, padding=0),\n",
    "                bn(self.in_channels)\n",
    "            )\n",
    "            nn.init.constant_(self.W[1].weight, 0)\n",
    "            nn.init.constant_(self.W[1].bias, 0)\n",
    "        else:\n",
    "            self.W = conv_nd(in_channels=self.inter_channels, out_channels=self.in_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "            nn.init.constant_(self.W.weight, 0)\n",
    "            nn.init.constant_(self.W.bias, 0)\n",
    "\n",
    "        self.theta = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                             kernel_size=1, stride=1, padding=0)\n",
    "        self.phi = conv_nd(in_channels=self.in_channels, out_channels=self.inter_channels,\n",
    "                           kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "        if sub_sample:\n",
    "            self.g = nn.Sequential(self.g, max_pool_layer)\n",
    "            self.phi = nn.Sequential(self.phi, max_pool_layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :param x: (b, c, t, h, w)\n",
    "        :return:\n",
    "        '''\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        g_x = self.g(x).view(batch_size, self.inter_channels, -1)\n",
    "        g_x = g_x.permute(0, 2, 1)\n",
    "\n",
    "        theta_x = self.theta(x).view(batch_size, self.inter_channels, -1)\n",
    "        theta_x = theta_x.permute(0, 2, 1)\n",
    "        phi_x = self.phi(x).view(batch_size, self.inter_channels, -1)\n",
    "        f = torch.matmul(theta_x, phi_x)\n",
    "        f_div_C = F.softmax(f, dim=-1)\n",
    "\n",
    "        y = torch.matmul(f_div_C, g_x)\n",
    "        y = y.permute(0, 2, 1).contiguous()\n",
    "        y = y.view(batch_size, self.inter_channels, *x.size()[2:])\n",
    "        W_y = self.W(y)\n",
    "        z = W_y + x\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class NONLocalBlock1D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock1D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=1, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)\n",
    "\n",
    "\n",
    "class NONLocalBlock2D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock2D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=2, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)\n",
    "\n",
    "\n",
    "class NONLocalBlock3D(_NonLocalBlockND):\n",
    "    def __init__(self, in_channels, inter_channels=None, sub_sample=True, bn_layer=True):\n",
    "        super(NONLocalBlock3D, self).__init__(in_channels,\n",
    "                                              inter_channels=inter_channels,\n",
    "                                              dimension=3, sub_sample=sub_sample,\n",
    "                                              bn_layer=bn_layer)\n",
    "\n",
    "\n",
    "class NL3DWrapper(nn.Module):\n",
    "    def __init__(self, block, n_segment):\n",
    "        super(NL3DWrapper, self).__init__()\n",
    "        self.block = block\n",
    "        self.nl = NONLocalBlock3D(block.bn3.num_features)\n",
    "        self.n_segment = n_segment\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block(x)\n",
    "\n",
    "        nt, c, h, w = x.size()\n",
    "        x = x.view(nt // self.n_segment, self.n_segment, c, h, w).transpose(1, 2)  # n, c, t, h, w\n",
    "        x = self.nl(x)\n",
    "        x = x.transpose(1, 2).contiguous().view(nt, c, h, w)\n",
    "        return x\n",
    "\n",
    "\n",
    "def make_non_local(net, n_segment):\n",
    "    import torchvision\n",
    "    if isinstance(net, torchvision.models.ResNet):\n",
    "        net.layer2 = nn.Sequential(\n",
    "            NL3DWrapper(net.layer2[0], n_segment),\n",
    "            net.layer2[1],\n",
    "            NL3DWrapper(net.layer2[2], n_segment),\n",
    "            net.layer2[3],\n",
    "        )\n",
    "        net.layer3 = nn.Sequential(\n",
    "            NL3DWrapper(net.layer3[0], n_segment),\n",
    "            net.layer3[1],\n",
    "            NL3DWrapper(net.layer3[2], n_segment),\n",
    "            net.layer3[3],\n",
    "            NL3DWrapper(net.layer3[4], n_segment),\n",
    "            net.layer3[5],\n",
    "        )\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from torch.autograd import Variable\n",
    "    import torch\n",
    "\n",
    "    sub_sample = True\n",
    "    bn_layer = True\n",
    "\n",
    "    img = Variable(torch.zeros(2, 3, 20))\n",
    "    net = NONLocalBlock1D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n",
    "    out = net(img)\n",
    "    print(out.size())\n",
    "\n",
    "    img = Variable(torch.zeros(2, 3, 20, 20))\n",
    "    net = NONLocalBlock2D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n",
    "    out = net(img)\n",
    "    print(out.size())\n",
    "\n",
    "    img = Variable(torch.randn(2, 3, 10, 20, 20))\n",
    "    net = NONLocalBlock3D(3, sub_sample=sub_sample, bn_layer=bn_layer)\n",
    "    out = net(img)\n",
    "    print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:47:41.196725Z",
     "iopub.status.busy": "2025-05-24T10:47:41.196438Z",
     "iopub.status.idle": "2025-05-24T10:47:41.202451Z",
     "shell.execute_reply": "2025-05-24T10:47:41.201838Z",
     "shell.execute_reply.started": "2025-05-24T10:47:41.196699Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Identity(torch.nn.Module):\n",
    "    def forward(self, input):\n",
    "        return input\n",
    "\n",
    "\n",
    "class SegmentConsensus(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, consensus_type, dim=1):\n",
    "        super(SegmentConsensus, self).__init__()\n",
    "        self.consensus_type = consensus_type\n",
    "        self.dim = dim\n",
    "        self.shape = None\n",
    "\n",
    "    def forward(self, input_tensor):\n",
    "        self.shape = input_tensor.size()\n",
    "        if self.consensus_type == 'avg':\n",
    "            output = input_tensor.mean(dim=self.dim, keepdim=True)\n",
    "        elif self.consensus_type == 'identity':\n",
    "            output = input_tensor\n",
    "        else:\n",
    "            output = None\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConsensusModule(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, consensus_type, dim=1):\n",
    "        super(ConsensusModule, self).__init__()\n",
    "        self.consensus_type = consensus_type if consensus_type != 'rnn' else 'identity'\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, input):\n",
    "        return SegmentConsensus(self.consensus_type, self.dim)(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:47:41.203554Z",
     "iopub.status.busy": "2025-05-24T10:47:41.203341Z",
     "iopub.status.idle": "2025-05-24T10:47:54.125778Z",
     "shell.execute_reply": "2025-05-24T10:47:54.123272Z",
     "shell.execute_reply.started": "2025-05-24T10:47:41.203537Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn.init import normal_, constant_\n",
    "\n",
    "\n",
    "class TSN(nn.Module):\n",
    "    def __init__(self, num_class, num_segments, modality, img_size = (224, 224),\n",
    "                 base_model='resnet101', new_length=None,\n",
    "                 consensus_type='avg', before_softmax=True,\n",
    "                 dropout=0.8, img_feature_dim=256,\n",
    "                 crop_num=1, partial_bn=True, print_spec=True, pretrain='imagenet',\n",
    "                 is_shift=False, shift_div=8, shift_place='blockres', fc_lr5=False,\n",
    "                 temporal_pool=False, non_local=False, data_length=1):\n",
    "        super(TSN, self).__init__()\n",
    "\n",
    "        self.TSM_intrada = STMEM(num_segments=num_segments, new_length=data_length, img_size = img_size)\n",
    "        self.modality = modality\n",
    "        self.img_size = img_size\n",
    "        self.num_segments = num_segments\n",
    "        self.reshape = True\n",
    "        self.before_softmax = before_softmax\n",
    "        self.dropout = dropout\n",
    "        self.crop_num = crop_num\n",
    "        self.consensus_type = consensus_type\n",
    "        self.img_feature_dim = img_feature_dim  # the dimension of the CNN feature to represent each frame\n",
    "        self.pretrain = pretrain\n",
    "\n",
    "        self.is_shift = is_shift\n",
    "        self.shift_div = shift_div\n",
    "        self.shift_place = shift_place\n",
    "        self.base_model_name = base_model\n",
    "        self.fc_lr5 = fc_lr5\n",
    "        self.temporal_pool = temporal_pool\n",
    "        self.non_local = non_local\n",
    "        self.data_length = data_length\n",
    "\n",
    "        if not before_softmax and consensus_type != 'avg':\n",
    "            raise ValueError(\"Only avg consensus can be used after Softmax\")\n",
    "\n",
    "        if new_length is None:\n",
    "            self.new_length = 1 if modality in [\"RGB\", 'depth', \"motion\", \"dense\"] else 5\n",
    "        else:\n",
    "            self.new_length = new_length\n",
    "        if print_spec:\n",
    "            print((\"\"\" \n",
    "    Initializing TSN with base model: {}.\n",
    "    TSN Configurations:\n",
    "        input_modality:     {}\n",
    "        num_segments:       {}\n",
    "        new_length:         {}\n",
    "        STMEM_new-length:   {}\n",
    "        consensus_module:   {}\n",
    "        dropout_ratio:      {}\n",
    "        img_feature_dim:    {}\n",
    "            \"\"\".format(base_model, self.modality, self.num_segments, self.new_length, self.data_length, consensus_type, self.dropout, self.img_feature_dim)))\n",
    "\n",
    "        self._prepare_base_model(base_model)\n",
    "\n",
    "        feature_dim = self._prepare_tsn(num_class)\n",
    "\n",
    "        if self.modality == 'Flow':\n",
    "            print(\"Converting the ImageNet model to a flow init model\")\n",
    "            self.base_model = self._construct_flow_model(self.base_model)\n",
    "            print(\"Done. Flow model ready...\")\n",
    "        elif self.modality == 'RGBDiff':\n",
    "            print(\"Converting the ImageNet model to RGB+Diff init model\")\n",
    "            self.base_model = self._construct_diff_model(self.base_model)\n",
    "            print(\"Done. RGBDiff model ready.\")\n",
    "\n",
    "        self.consensus = ConsensusModule(consensus_type)\n",
    "\n",
    "        if not self.before_softmax:\n",
    "            self.softmax = nn.Softmax()\n",
    "\n",
    "        self._enable_pbn = partial_bn\n",
    "        if partial_bn:\n",
    "            self.partialBN(True)\n",
    "\n",
    "    def _prepare_tsn(self, num_class):\n",
    "        feature_dim = getattr(self.base_model, self.base_model.last_layer_name).in_features\n",
    "        if self.dropout == 0:\n",
    "            setattr(self.base_model, self.base_model.last_layer_name, nn.Linear(feature_dim, num_class))\n",
    "            self.new_fc = None\n",
    "        else:\n",
    "            setattr(self.base_model, self.base_model.last_layer_name, nn.Dropout(p=self.dropout))\n",
    "            self.new_fc = nn.Linear(feature_dim, num_class)\n",
    "\n",
    "        std = 0.001\n",
    "        if self.new_fc is None:\n",
    "            normal_(getattr(self.base_model, self.base_model.last_layer_name).weight, 0, std)\n",
    "            constant_(getattr(self.base_model, self.base_model.last_layer_name).bias, 0)\n",
    "        else:\n",
    "            if hasattr(self.new_fc, 'weight'):\n",
    "                normal_(self.new_fc.weight, 0, std)\n",
    "                constant_(self.new_fc.bias, 0)\n",
    "        return feature_dim\n",
    "\n",
    "    def _prepare_base_model(self, base_model):\n",
    "        print('=> base model: {}'.format(base_model))\n",
    "\n",
    "        if 'resnet' in base_model:\n",
    "            import torchvision\n",
    "            self.base_model = getattr(torchvision.models, base_model)(True if self.pretrain == 'imagenet' else False)\n",
    "            if self.is_shift:\n",
    "                print('Adding temporal shift...')\n",
    "                make_temporal_shift(self.base_model, self.num_segments,\n",
    "                                    n_div=self.shift_div, place=self.shift_place, temporal_pool=self.temporal_pool)\n",
    "\n",
    "            if self.non_local:\n",
    "                print('Adding non-local module...')\n",
    "                make_non_local(self.base_model, self.num_segments)\n",
    "\n",
    "            self.base_model.last_layer_name = 'fc'\n",
    "            self.input_size = self.img_size[0]\n",
    "            self.input_mean = [0.485, 0.456, 0.406]\n",
    "            self.input_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "            self.base_model.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "            if self.modality == 'Flow':\n",
    "                self.input_mean = [0.5]\n",
    "                self.input_std = [np.mean(self.input_std)]\n",
    "            elif self.modality == 'RGBDiff':\n",
    "                self.input_mean = [0.485, 0.456, 0.406] + [0] * 3 * self.new_length\n",
    "                self.input_std = self.input_std + [np.mean(self.input_std) * 2] * 3 * self.new_length\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Unknown base model: {}\".format(base_model))\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"\n",
    "        Override the default train() to freeze the BN parameters\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        super(TSN, self).train(mode)\n",
    "        count = 0\n",
    "        if self._enable_pbn and mode:\n",
    "            print(\"Freezing BatchNorm2D except the first one.\")\n",
    "            for m in self.base_model.modules():\n",
    "                if isinstance(m, nn.BatchNorm2d):\n",
    "                    count += 1\n",
    "                    if count >= (2 if self._enable_pbn else 1):\n",
    "                        m.eval()\n",
    "                        # shutdown update in frozen mode\n",
    "                        m.weight.requires_grad = False\n",
    "                        m.bias.requires_grad = False\n",
    "\n",
    "    def partialBN(self, enable):\n",
    "        self._enable_pbn = enable\n",
    "\n",
    "    def get_optim_policies(self):\n",
    "        first_conv_weight = []\n",
    "        first_conv_bias = []\n",
    "        normal_weight = []\n",
    "        normal_bias = []\n",
    "        lr5_weight = []\n",
    "        lr10_bias = []\n",
    "        bn = []\n",
    "        custom_ops = []\n",
    "\n",
    "        conv_cnt = 0\n",
    "        bn_cnt = 0\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Conv1d) or isinstance(m, torch.nn.Conv3d):\n",
    "                ps = list(m.parameters())\n",
    "                conv_cnt += 1\n",
    "                if conv_cnt == 1:\n",
    "                    first_conv_weight.append(ps[0])\n",
    "                    if len(ps) == 2:\n",
    "                        first_conv_bias.append(ps[1])\n",
    "                else:\n",
    "                    normal_weight.append(ps[0])\n",
    "                    if len(ps) == 2:\n",
    "                        normal_bias.append(ps[1])\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                ps = list(m.parameters())\n",
    "                if self.fc_lr5:\n",
    "                    lr5_weight.append(ps[0])\n",
    "                else:\n",
    "                    normal_weight.append(ps[0])\n",
    "                if len(ps) == 2:\n",
    "                    if self.fc_lr5:\n",
    "                        lr10_bias.append(ps[1])\n",
    "                    else:\n",
    "                        normal_bias.append(ps[1])\n",
    "\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                bn_cnt += 1\n",
    "                # later BN's are frozen\n",
    "                if not self._enable_pbn or bn_cnt == 1:\n",
    "                    bn.extend(list(m.parameters()))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                bn_cnt += 1\n",
    "                # later BN's are frozen\n",
    "                if not self._enable_pbn or bn_cnt == 1:\n",
    "                    bn.extend(list(m.parameters()))\n",
    "            elif len(m._modules) == 0:\n",
    "                if len(list(m.parameters())) > 0:\n",
    "                    raise ValueError(\"New atomic module type: {}. Need to give it a learning policy\".format(type(m)))\n",
    "\n",
    "        return [\n",
    "            {'params': first_conv_weight, 'lr_mult': 5 if self.modality == 'Flow' else 1, 'decay_mult': 1,\n",
    "             'name': \"first_conv_weight\"},\n",
    "            {'params': first_conv_bias, 'lr_mult': 10 if self.modality == 'Flow' else 2, 'decay_mult': 0,\n",
    "             'name': \"first_conv_bias\"},\n",
    "            {'params': normal_weight, 'lr_mult': 1, 'decay_mult': 1,\n",
    "             'name': \"normal_weight\"},\n",
    "            {'params': normal_bias, 'lr_mult': 2, 'decay_mult': 0,\n",
    "             'name': \"normal_bias\"},\n",
    "            {'params': bn, 'lr_mult': 1, 'decay_mult': 0,\n",
    "             'name': \"BN scale/shift\"},\n",
    "            {'params': custom_ops, 'lr_mult': 1, 'decay_mult': 1,\n",
    "             'name': \"custom_ops\"},\n",
    "            # for fc\n",
    "            {'params': lr5_weight, 'lr_mult': 5, 'decay_mult': 1,\n",
    "             'name': \"lr5_weight\"},\n",
    "            {'params': lr10_bias, 'lr_mult': 10, 'decay_mult': 0,\n",
    "             'name': \"lr10_bias\"},\n",
    "        ]\n",
    "\n",
    "    def forward(self, input, no_reshape=False):\n",
    "\n",
    "        output_intrada = self.TSM_intrada(input)#\n",
    "        base_out = self.base_model(output_intrada)\n",
    "\n",
    "        # print('base out', base_out.size())\n",
    "\n",
    "        if self.dropout > 0:\n",
    "            base_out = self.new_fc(base_out)\n",
    "            # print('after last', base_out.size())\n",
    "\n",
    "        if not self.before_softmax:\n",
    "            base_out = self.softmax(base_out)\n",
    "\n",
    "        if self.reshape:\n",
    "            # print('In reshape')\n",
    "            if self.is_shift and self.temporal_pool:\n",
    "                # print('use shift')\n",
    "                base_out = base_out.view((-1, self.num_segments // 2) + base_out.size()[1:])\n",
    "                # print('Out', base_out.size())\n",
    "            else:\n",
    "                # print('NO use shift')\n",
    "                \n",
    "                base_out = base_out.view((-1, self.num_segments) + base_out.size()[1:])\n",
    "            output = self.consensus(base_out)\n",
    "            return output.squeeze(1)\n",
    "\n",
    "    def _construct_flow_model(self, base_model):\n",
    "        # modify the convolution layers\n",
    "        modules = list(self.base_model.modules())\n",
    "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
    "        conv_layer = modules[first_conv_idx]\n",
    "        container = modules[first_conv_idx - 1]\n",
    "\n",
    "        # modify parameters, assume the first blob contains the convolution kernels\n",
    "        params = [x.clone() for x in conv_layer.parameters()]\n",
    "        kernel_size = params[0].size()\n",
    "        new_kernel_size = kernel_size[:1] + (2 * self.new_length, ) + kernel_size[2:]\n",
    "        new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
    "\n",
    "        new_conv = nn.Conv2d(2 * self.new_length, conv_layer.out_channels,\n",
    "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
    "                             bias=True if len(params) == 2 else False)\n",
    "        new_conv.weight.data = new_kernels\n",
    "        if len(params) == 2:\n",
    "            new_conv.bias.data = params[1].data # add bias if neccessary\n",
    "        layer_name = list(container.state_dict().keys())[0][:-7] # remove .weight suffix to get the layer name\n",
    "\n",
    "        # replace the first convlution layer\n",
    "        setattr(container, layer_name, new_conv)\n",
    "\n",
    "        return base_model\n",
    "\n",
    "    def _construct_diff_model(self, base_model, keep_rgb=False):\n",
    "        # modify the convolution layers\n",
    "        modules = list(self.base_model.modules())\n",
    "        first_conv_idx = list(filter(lambda x: isinstance(modules[x], nn.Conv2d), list(range(len(modules)))))[0]\n",
    "\n",
    "        conv_layer = modules[first_conv_idx]\n",
    "        container = modules[first_conv_idx - 1]\n",
    "\n",
    "        # modify parameters, assume the first blob contains the convolution kernels\n",
    "        params = [x.clone() for x in conv_layer.parameters()]\n",
    "        kernel_size = params[0].size()\n",
    "        if not keep_rgb:\n",
    "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
    "            new_kernels = params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()\n",
    "        else:\n",
    "            new_kernel_size = kernel_size[:1] + (3 * self.new_length,) + kernel_size[2:]\n",
    "            new_kernels = torch.cat((params[0].data, params[0].data.mean(dim=1, keepdim=True).expand(new_kernel_size).contiguous()),\n",
    "                                    1)\n",
    "            new_kernel_size = kernel_size[:1] + (3 + 3 * self.new_length,) + kernel_size[2:]\n",
    "\n",
    "        new_conv = nn.Conv2d(new_kernel_size[1], conv_layer.out_channels,\n",
    "                             conv_layer.kernel_size, conv_layer.stride, conv_layer.padding,\n",
    "                             bias=True if len(params) == 2 else False)\n",
    "        new_conv.weight.data = new_kernels\n",
    "        if len(params) == 2:\n",
    "            new_conv.bias.data = params[1].data  # add bias if neccessary\n",
    "        layer_name = list(container.state_dict().keys())[0][:-7]  # remove .weight suffix to get the layer name\n",
    "\n",
    "        # replace the first convolution layer\n",
    "        setattr(container, layer_name, new_conv)\n",
    "        return base_model\n",
    "\n",
    "\n",
    "# if __name__== '__main__':\n",
    "#     from ptflops import get_model_complexity_info\n",
    "#     import torch\n",
    "#     x1 = torch.rand((4, 144, 224, 224))\n",
    "#     net = TSN(num_class=60,num_segments=8,base_model='resnet50',modality='RGB',consensus_type='avg',\n",
    "#                 dropout=0.8,\n",
    "#                 img_feature_dim=256,\n",
    "#                 partial_bn=False,\n",
    "#                 pretrain='imagenet',\n",
    "#                 is_shift=True, shift_div=8, shift_place='blockres',\n",
    "#                 fc_lr5=True,\n",
    "#                 temporal_pool=False,\n",
    "#                 non_local=False, data_length=6)\n",
    "#     # print(net)\n",
    "#     output = net(x1)\n",
    "#     print(output.size())\n",
    "#     policies = net.get_optim_policies()\n",
    "#     for group in policies:\n",
    "#         print(('group: {} has {} params, lr_mult: {}, decay_mult: {}'.format(\n",
    "#             group['name'], len(group['params']), group['lr_mult'], group['decay_mult'])))\n",
    "\n",
    "#     flops, params = get_model_complexity_info(net, (144, 224, 224), as_strings=True,\n",
    "#                                               print_per_layer_stat=True)  # as_strings=True,会用G或M为单位，反之精确到个位。\n",
    "#     print(\"%s |%s |%s\" % ('net', flops, params))#stmem |33.35 GMac |23.63 M   TSM原版 net |32.96 GMac |23.63 M      stiam  net |33.16 GMac |23.63 M   seg=4  net |16.68 GMac |23.63 M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:57:48.977896Z",
     "iopub.status.busy": "2025-05-24T10:57:48.977385Z",
     "iopub.status.idle": "2025-05-24T10:57:51.327907Z",
     "shell.execute_reply": "2025-05-24T10:57:51.327297Z",
     "shell.execute_reply.started": "2025-05-24T10:57:48.977872Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# def debug_model(model, device):\n",
    "#     model.to(device)\n",
    "#     model.eval()\n",
    "\n",
    "#     # Simulate dummy input and label\n",
    "#     batch_size = 64\n",
    "#     num_segments = model.num_segments  # e.g., 6\n",
    "#     stmem_len = model.data_length\n",
    "#     C, H, W = 3, 128, 128\n",
    "#     num_classes = model.num_classes if hasattr(model, \"num_classes\") else 10\n",
    "\n",
    "#     # Shape: [B, num_segments, C, H, W]\n",
    "#     video = torch.randn(batch_size, num_segments*stmem_len*C, H, W).to(device)\n",
    "#     label = torch.randint(0, num_classes, (batch_size,)).to(device)\n",
    "\n",
    "#     print(\"Input video shape:\", video.shape)\n",
    "#     print(\"Label shape:\", label.shape)\n",
    "\n",
    "#     # Forward pass\n",
    "#     with torch.no_grad():\n",
    "#         output = model(video)\n",
    "#         print(\"Model output shape:\", output.shape)\n",
    "\n",
    "#         # Check loss computation\n",
    "#         loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#         try:\n",
    "#             loss = loss_fn(output, label)\n",
    "#             print(\"✅ Loss computed successfully:\", loss.item())\n",
    "#         except Exception as e:\n",
    "#             print(\"❌ Error during loss computation:\", e)\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     num_segments = 6\n",
    "#     INPUT_DEPTH = 36\n",
    "\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model = TSN(num_class = 18, num_segments = num_segments, modality = \"RGBDiff\", img_size = (128, 128),\n",
    "#                 new_length = 1,\n",
    "#                 base_model=\"resnet50\",\n",
    "#                 dropout= 0.5,\n",
    "#                 img_feature_dim=128,\n",
    "#                 pretrain= \"imagenet\",\n",
    "#                 is_shift= True,\n",
    "#                 # fc_lr5=not (args.tune_from and args.dataset in args.tune_from),\n",
    "#                 fc_lr5=True,\n",
    "#                 temporal_pool=False,\n",
    "#                 non_local=True, data_length = int(INPUT_DEPTH/num_segments))\n",
    "\n",
    "#     debug_model(model, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data frame processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T10:48:15.905063Z",
     "iopub.status.busy": "2025-05-24T10:48:15.904323Z",
     "iopub.status.idle": "2025-05-24T10:48:17.685433Z",
     "shell.execute_reply": "2025-05-24T10:48:17.684804Z",
     "shell.execute_reply.started": "2025-05-24T10:48:15.905030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.130216Z",
     "iopub.status.idle": "2025-05-24T10:47:54.130556Z",
     "shell.execute_reply": "2025-05-24T10:47:54.130408Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.130381Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.131677Z",
     "iopub.status.idle": "2025-05-24T10:47:54.131938Z",
     "shell.execute_reply": "2025-05-24T10:47:54.131814Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.131804Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = \"/kaggle/input/20-bnjester-csv-files/Train.csv\"\n",
    "VAL_DATA_DIR = \"/kaggle/input/20-bnjester-csv-files/Validation.csv\"\n",
    "Cropped_TRAIN_DIR = \"/kaggle/input/hand-cropped-20jester-train-dataset/Cropped_Train_Data\"\n",
    "Cropped_VAL_DIR = \"/kaggle/input/hand-cropped-20jester-validation-dataset/Cropped_Validation_Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.133376Z",
     "iopub.status.idle": "2025-05-24T10:47:54.133600Z",
     "shell.execute_reply": "2025-05-24T10:47:54.133510Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.133500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# label_encoder = OneHotEncoder(sparse_output=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.134384Z",
     "iopub.status.idle": "2025-05-24T10:47:54.134727Z",
     "shell.execute_reply": "2025-05-24T10:47:54.134574Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.134558Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gesture_names = [\n",
    "    \"Doing other things\", \"No gesture\",\n",
    "    \"Rolling Hand Backward\", \"Rolling Hand Forward\",\n",
    "    \"Shaking Hand\",\n",
    "    \"Sliding Two Fingers Down\", \"Sliding Two Fingers Left\", \n",
    "    \"Sliding Two Fingers Right\", \"Sliding Two Fingers Up\",\n",
    "    \"Stop Sign\",\n",
    "    \"Swiping Down\", \"Swiping Left\", \"Swiping Right\", \"Swiping Up\",\n",
    "    \"Thumb Down\", \"Thumb Up\",\n",
    "    \"Turning Hand Clockwise\", \"Turning Hand Counterclockwise\"\n",
    "]\n",
    "\n",
    "label_to_index = {label: idx for idx, label in enumerate(gesture_names)}\n",
    "\n",
    "# Example usage:\n",
    "# If you have a label \"Swiping Left\", get its integer target for CrossEntropyLoss:\n",
    "label_to_index[\"Swiping Left\"]  # returns 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.135799Z",
     "iopub.status.idle": "2025-05-24T10:47:54.136060Z",
     "shell.execute_reply": "2025-05-24T10:47:54.135956Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.135944Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_video_id_ls = list(map(int, os.listdir(Cropped_TRAIN_DIR)))\n",
    "val_video_id_ls = list(map(int, os.listdir(Cropped_VAL_DIR)))\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_DIR)\n",
    "val_df = pd.read_csv(VAL_DATA_DIR)\n",
    "\n",
    "sort_train_df = train_df[train_df[\"video_id\"].isin(train_video_id_ls)] # sorting only the used data\n",
    "sort_val_df = val_df[val_df[\"video_id\"].isin(val_video_id_ls)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.136857Z",
     "iopub.status.idle": "2025-05-24T10:47:54.137104Z",
     "shell.execute_reply": "2025-05-24T10:47:54.137006Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.136996Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"train_df len: \", len(train_df))\n",
    "print(\"sort_train_df len: \", len(sort_train_df))\n",
    "print(\"val_df len: \", len(val_df))\n",
    "print(\"sort_val_df len: \", len(sort_val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.139065Z",
     "iopub.status.idle": "2025-05-24T10:47:54.139458Z",
     "shell.execute_reply": "2025-05-24T10:47:54.139324Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.139304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.140033Z",
     "iopub.status.idle": "2025-05-24T10:47:54.140639Z",
     "shell.execute_reply": "2025-05-24T10:47:54.140206Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.140176Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "num_segments = 6\n",
    "INPUT_DEPTH = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.141388Z",
     "iopub.status.idle": "2025-05-24T10:47:54.141626Z",
     "shell.execute_reply": "2025-05-24T10:47:54.141513Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.141504Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_model(device):\n",
    "    model = TSN(num_class = 18, num_segments = num_segments, modality = \"RGBDiff\", img_size = (128, 128),\n",
    "                new_length = 1,\n",
    "                base_model=\"resnet50\",\n",
    "                dropout= 0.5,\n",
    "                img_feature_dim=128,\n",
    "                pretrain= \"imagenet\",\n",
    "                is_shift= True,\n",
    "                # fc_lr5=not (args.tune_from and args.dataset in args.tune_from),\n",
    "                fc_lr5=True,\n",
    "                temporal_pool=False,\n",
    "                non_local=True, data_length = int(INPUT_DEPTH/num_segments))\n",
    "    \n",
    "    model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.143013Z",
     "iopub.status.idle": "2025-05-24T10:47:54.143222Z",
     "shell.execute_reply": "2025-05-24T10:47:54.143133Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.143124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = create_model(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.144384Z",
     "iopub.status.idle": "2025-05-24T10:47:54.144632Z",
     "shell.execute_reply": "2025-05-24T10:47:54.144491Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.144481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.145207Z",
     "iopub.status.idle": "2025-05-24T10:47:54.145495Z",
     "shell.execute_reply": "2025-05-24T10:47:54.145393Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.145380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.core.composition import ReplayCompose\n",
    "from albumentations.pytorch.transforms import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.147007Z",
     "iopub.status.idle": "2025-05-24T10:47:54.147285Z",
     "shell.execute_reply": "2025-05-24T10:47:54.147187Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.147175Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "\n",
    "class MultiScaleCropAlbumentations(ImageOnlyTransform):\n",
    "    def __init__(self, input_size, scales=None, max_distort=1, fix_crop=True, more_fix_crop=True, interpolation=cv2.INTER_LINEAR, always_apply=False, p=1.0):\n",
    "        super().__init__(always_apply, p)\n",
    "        self.scales = scales if scales is not None else [1, .875, .75, .66]\n",
    "        self.max_distort = max_distort\n",
    "        self.fix_crop = fix_crop\n",
    "        self.more_fix_crop = more_fix_crop\n",
    "        self.input_size = input_size if not isinstance(input_size, int) else [input_size, input_size]\n",
    "        self.interpolation = interpolation\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        im_h, im_w = img.shape[:2]\n",
    "        crop_w, crop_h, offset_w, offset_h = self._sample_crop_size((im_w, im_h))\n",
    "\n",
    "        # Crop and resize\n",
    "        cropped = img[offset_h:offset_h + crop_h, offset_w:offset_w + crop_w]\n",
    "        resized = cv2.resize(cropped, (self.input_size[0], self.input_size[1]), interpolation=self.interpolation)\n",
    "        return resized\n",
    "\n",
    "    def _sample_crop_size(self, im_size):\n",
    "        image_w, image_h = im_size\n",
    "        base_size = min(image_w, image_h)\n",
    "\n",
    "        crop_sizes = [int(base_size * x) for x in self.scales]\n",
    "        crop_h = [self.input_size[1] if abs(x - self.input_size[1]) < 3 else x for x in crop_sizes]\n",
    "        crop_w = [self.input_size[0] if abs(x - self.input_size[0]) < 3 else x for x in crop_sizes]\n",
    "\n",
    "        pairs = []\n",
    "        for i, h in enumerate(crop_h):\n",
    "            for j, w in enumerate(crop_w):\n",
    "                if abs(i - j) <= self.max_distort:\n",
    "                    pairs.append((w, h))\n",
    "\n",
    "        crop_pair = random.choice(pairs)\n",
    "\n",
    "        if not self.fix_crop:\n",
    "            w_offset = random.randint(0, image_w - crop_pair[0])\n",
    "            h_offset = random.randint(0, image_h - crop_pair[1])\n",
    "        else:\n",
    "            w_offset, h_offset = self._sample_fix_offset(image_w, image_h, crop_pair[0], crop_pair[1])\n",
    "\n",
    "        return crop_pair[0], crop_pair[1], w_offset, h_offset\n",
    "\n",
    "    def _sample_fix_offset(self, image_w, image_h, crop_w, crop_h):\n",
    "        offsets = self.fill_fix_offset(self.more_fix_crop, image_w, image_h, crop_w, crop_h)\n",
    "        return random.choice(offsets)\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_fix_offset(more_fix_crop, image_w, image_h, crop_w, crop_h):\n",
    "        w_step = (image_w - crop_w) // 4\n",
    "        h_step = (image_h - crop_h) // 4\n",
    "\n",
    "        ret = [\n",
    "            (0, 0),\n",
    "            (4 * w_step, 0),\n",
    "            (0, 4 * h_step),\n",
    "            (4 * w_step, 4 * h_step),\n",
    "            (2 * w_step, 2 * h_step),\n",
    "        ]\n",
    "\n",
    "        if more_fix_crop:\n",
    "            ret += [\n",
    "                (0, 2 * h_step),\n",
    "                (4 * w_step, 2 * h_step),\n",
    "                (2 * w_step, 4 * h_step),\n",
    "                (2 * w_step, 0),\n",
    "                (1 * w_step, 1 * h_step),\n",
    "                (3 * w_step, 1 * h_step),\n",
    "                (1 * w_step, 3 * h_step),\n",
    "                (3 * w_step, 3 * h_step),\n",
    "            ]\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.148403Z",
     "iopub.status.idle": "2025-05-24T10:47:54.148658Z",
     "shell.execute_reply": "2025-05-24T10:47:54.148534Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.148524Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def augmentation(img, input_shape):\n",
    "    \"\"\"Augmentation function for albumentations.\"\"\"\n",
    "\n",
    "    transform = ReplayCompose([\n",
    "        A.RandomBrightnessContrast(p=0.2),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.Resize(height=input_shape[0], width=input_shape[1]),\n",
    "        # MultiScaleCropAlbumentations(input_size=input_shape, scales = [1, .875, .75]),\n",
    "    ])\n",
    "\n",
    "    aug = transform(image=img)\n",
    "\n",
    "    return aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.149560Z",
     "iopub.status.idle": "2025-05-24T10:47:54.149771Z",
     "shell.execute_reply": "2025-05-24T10:47:54.149674Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.149666Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# input_mean = model.module.input_mean\n",
    "# input_std = model.module.input_std\n",
    "\n",
    "# print(input_mean)\n",
    "# print(input_std)\n",
    "\n",
    "\n",
    "# def normalize(image):\n",
    "\n",
    "#     transform = ReplayCompose([\n",
    "#         A.Normalize(mean=input_mean, std=input_std),\n",
    "#     ])\n",
    "\n",
    "#     aug = transform(image=image)\n",
    "\n",
    "#     return aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.151130Z",
     "iopub.status.idle": "2025-05-24T10:47:54.151336Z",
     "shell.execute_reply": "2025-05-24T10:47:54.151248Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.151240Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VideoDatasetTorch(Dataset):\n",
    "    def __init__(self, usage, data_frame, video_dir, input_shape, label_to_index, transform=None):\n",
    "        self.usage = usage\n",
    "        self.data = data_frame.reset_index(drop=True)\n",
    "        self.video_dir = video_dir\n",
    "        self.input_shape = input_shape  # (D, H, W, C)\n",
    "        self.transform = transform\n",
    "        self.label_to_index = label_to_index\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        video_id = row.video_id\n",
    "        label_str = row.label\n",
    "        folder = os.path.join(self.video_dir, str(video_id))\n",
    "\n",
    "        frames = []\n",
    "        replay = None\n",
    "\n",
    "        for j in range(1, 37):\n",
    "            img_path = os.path.join(folder, f\"{j:05d}.jpg\")\n",
    "            img = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "            if self.usage == \"train\" and self.transform:\n",
    "                if replay is None:\n",
    "                    aug = self.transform(img=img, input_shape=(self.input_shape[1], self.input_shape[2]))\n",
    "                    replay = aug[\"replay\"]\n",
    "                    aug_img = aug[\"image\"]\n",
    "                else:\n",
    "                    aug_img = ReplayCompose.replay(replay, image=img)[\"image\"]\n",
    "                \n",
    "            else:\n",
    "                aug_img = cv2.resize(img, (self.input_shape[2], self.input_shape[1]))\n",
    "                \n",
    "            aug_img = aug_img.astype(np.float32) / 255.0\n",
    "            frames.append(aug_img)\n",
    "\n",
    "        # Convert to (T, H, W, C) then to (T, C, H, W)\n",
    "        video = np.stack(frames, axis=0).transpose(0, 3, 1, 2)  # (T, C, H, W)\n",
    "        \n",
    "        # Flatten temporal and channel dims: (T*C, H, W) --> torch.Size([S * L * 3, H, W])\n",
    "        video = video.reshape(-1, self.input_shape[1], self.input_shape[2])  # (T*C, H, W)\n",
    "        video_tensor = torch.tensor(video, dtype=torch.float32)\n",
    "    \n",
    "        # Convert label string to integer index using the provided dictionary\n",
    "        label_index = self.label_to_index[label_str]\n",
    "        label_tensor = torch.tensor(label_index, dtype=torch.long)\n",
    "\n",
    "        return video_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.152369Z",
     "iopub.status.idle": "2025-05-24T10:47:54.152602Z",
     "shell.execute_reply": "2025-05-24T10:47:54.152501Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.152491Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset = VideoDatasetTorch(\n",
    "    usage=\"train\",\n",
    "    data_frame=sort_train_df,\n",
    "    video_dir=Cropped_TRAIN_DIR,\n",
    "    input_shape=(36, 128, 128, 3),\n",
    "    # normalize = normalize,\n",
    "    label_to_index = label_to_index,\n",
    "    transform=augmentation\n",
    ")\n",
    "\n",
    "val_dataset = VideoDatasetTorch(\n",
    "    usage=\"val\",\n",
    "    data_frame=sort_val_df,\n",
    "    video_dir=Cropped_VAL_DIR,\n",
    "    input_shape=(36, 128, 128, 3),\n",
    "    # normalize = normalize,\n",
    "    label_to_index = label_to_index,\n",
    "    transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.153500Z",
     "iopub.status.idle": "2025-05-24T10:47:54.153775Z",
     "shell.execute_reply": "2025-05-24T10:47:54.153672Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.153659Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.154491Z",
     "iopub.status.idle": "2025-05-24T10:47:54.154808Z",
     "shell.execute_reply": "2025-05-24T10:47:54.154656Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.154642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = next(iter(train_loader))\n",
    "video, abs_ = data\n",
    "print(video.shape, abs_.shape)\n",
    "print(abs_)\n",
    "\n",
    "print(len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparaneters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.155653Z",
     "iopub.status.idle": "2025-05-24T10:47:54.155968Z",
     "shell.execute_reply": "2025-05-24T10:47:54.155813Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.155801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.157075Z",
     "iopub.status.idle": "2025-05-24T10:47:54.157367Z",
     "shell.execute_reply": "2025-05-24T10:47:54.157257Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.157243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PROJECT = \"HandActionReg\"\n",
    "RESUME = \"allow\"\n",
    "WANDB_KEY = \"d9d14819dddd8a35a353b5c0b087e0f60d717140\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.158353Z",
     "iopub.status.idle": "2025-05-24T10:47:54.158589Z",
     "shell.execute_reply": "2025-05-24T10:47:54.158475Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.158463Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.login(\n",
    "    key = WANDB_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.159698Z",
     "iopub.status.idle": "2025-05-24T10:47:54.160132Z",
     "shell.execute_reply": "2025-05-24T10:47:54.159987Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.159971Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-5\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.161481Z",
     "iopub.status.idle": "2025-05-24T10:47:54.161735Z",
     "shell.execute_reply": "2025-05-24T10:47:54.161599Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.161590Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=PROJECT,\n",
    "    resume=RESUME,\n",
    "    name=\"STMEM_hand_TSM_init\",\n",
    "    config={\n",
    "         \"learning_rate\": learning_rate,\n",
    "         \"epochs\": epochs,\n",
    "         \"batch_size\": batch_size,\n",
    "    },\n",
    ")\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.162575Z",
     "iopub.status.idle": "2025-05-24T10:47:54.162959Z",
     "shell.execute_reply": "2025-05-24T10:47:54.162814Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.162801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "policies = model.module.get_optim_policies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.163842Z",
     "iopub.status.idle": "2025-05-24T10:47:54.164153Z",
     "shell.execute_reply": "2025-05-24T10:47:54.163993Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.163981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "param_groups = []\n",
    "for group in policies:\n",
    "    param_groups.append({\n",
    "        'params': group['params'],\n",
    "        'lr': learning_rate * group.get('lr_mult', 1),\n",
    "        'weight_decay': weight_decay * group.get('decay_mult', 1)\n",
    "    })\n",
    "\n",
    "# print(param_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.165401Z",
     "iopub.status.idle": "2025-05-24T10:47:54.165621Z",
     "shell.execute_reply": "2025-05-24T10:47:54.165527Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.165518Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(param_groups, lr=learning_rate, weight_decay = weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.166707Z",
     "iopub.status.idle": "2025-05-24T10:47:54.166945Z",
     "shell.execute_reply": "2025-05-24T10:47:54.166828Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.166820Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "patience = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.167661Z",
     "iopub.status.idle": "2025-05-24T10:47:54.167902Z",
     "shell.execute_reply": "2025-05-24T10:47:54.167813Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.167803Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.0):\n",
    "        \"\"\"\n",
    "        Early stopping to terminate training when validation loss stops improving.\n",
    "\n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait before stopping after no improvement.\n",
    "            min_delta (float): Minimum change in the monitored value to be considered as an improvement.\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        \"\"\"\n",
    "        Call this function at the end of each validation step.\n",
    "\n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if training should stop, False otherwise.\n",
    "        \"\"\"\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0  # Reset counter if validation loss improves\n",
    "        else:\n",
    "            self.counter += 1  # Increase counter if no improvement\n",
    "\n",
    "        return self.counter >= self.patience  # Stop training if patience is exceeded\n",
    "        \n",
    "\n",
    "early_stopping = EarlyStopping(patience=patience, min_delta=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.169515Z",
     "iopub.status.idle": "2025-05-24T10:47:54.169839Z",
     "shell.execute_reply": "2025-05-24T10:47:54.169680Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.169667Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, min_lr=1e-6, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-24T10:47:54.170707Z",
     "iopub.status.idle": "2025-05-24T10:47:54.171036Z",
     "shell.execute_reply": "2025-05-24T10:47:54.170875Z",
     "shell.execute_reply.started": "2025-05-24T10:47:54.170862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, loss_fn, DEVICE):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    for video, label in tqdm(dataloader, desc=\"Training\", leave=False):\n",
    "        video = video.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = model(video)\n",
    "        loss = loss_fn(output, label)\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # Add gradient clipping here\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += label.size(0)\n",
    "        correct += (predicted == label).sum().item()\n",
    "        all_labels.extend(label.cpu().numpy())\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = correct / total\n",
    "    epoch_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    epoch_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    wandb.log({\n",
    "        \"loss\": epoch_loss,\n",
    "        \"accuracy\": epoch_acc,\n",
    "        \"precision\": epoch_precision,\n",
    "        \"recall\": epoch_recall\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_precision, epoch_recall\n",
    "\n",
    "def validate_epoch(model, dataloader, loss_fn, DEVICE):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for video, label in tqdm(dataloader, desc=\"Validation\", leave=False):\n",
    "            video = video.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "\n",
    "            output = model(video)\n",
    "            loss = loss_fn(output, label)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += label.size(0)\n",
    "            correct += (predicted == label).sum().item()\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_acc = correct / total\n",
    "    epoch_precision = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    epoch_recall = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "\n",
    "    wandb.log({\n",
    "        \"val_loss\": epoch_loss,\n",
    "        \"val_accuracy\": epoch_acc,\n",
    "        \"val_precision\": epoch_precision,\n",
    "        \"val_recall\": epoch_recall\n",
    "    })\n",
    "\n",
    "    return epoch_loss, epoch_acc, epoch_precision, epoch_recall\n",
    "\n",
    "# Training loop with early stopping and ReduceLROnPlateau\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, train_prec, train_rec = train_epoch(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss, val_acc, val_prec, val_rec = validate_epoch(model, val_loader, loss_fn, device)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_loss:.4f} Acc: {train_acc:.2f} | Val Loss: {val_loss:.4f} Acc: {val_acc:.2f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), \"/kaggle/working/STMEM_TSM_RestNet50.pth\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if early_stopping(val_loss):\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Save model at the last epoch\n",
    "torch.save(model.state_dict(), \"/kaggle/working/STMEM_TSM_RestNet50_last.pth\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7136783,
     "sourceId": 11395349,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7137173,
     "sourceId": 11396014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7254045,
     "sourceId": 11570522,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
